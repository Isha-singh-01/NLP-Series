{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f8e522a-e2dc-4304-b498-4428c78d24bd",
   "metadata": {},
   "source": [
    "## Feature Extraction\r\n",
    "\r\n",
    "This notebook discusses the various methods of feature extraction. It is also known as Text Vectorization or text representation. This technique is used to convert the words into vectors so that they can be used for modeling. The various techniques are as following of semantic meaning.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f22ab2-1793-438c-8e83-22d70dfebe8d",
   "metadata": {},
   "source": [
    "### 1. One hot encoding: \n",
    "\n",
    "Most basic technique to convert words into vectors. Consider the following example.\n",
    "- Doc1 = \"It is raining\"\n",
    "- Doc2 = \"Boston is raining\"\n",
    "- Doc3 = \"Rain raining everywhere.\"\n",
    "\n",
    "Vocabulary of this corpus = [\"It\",\"is\",\"raining\",\"Boston\",\"rain\",\"everywhere\"].\n",
    "In the vector form these docs can be converted as:\n",
    "    \n",
    "```\n",
    "Doc1 = [[1,0,0,0,0,0],[0,1,0,0,0,0],[0,0,1,0,0,0]] -- size of this vector is 3X6.\n",
    "Doc2 = [[0,0,0,1,0,0],[0,1,0,0,0,0],[0,0,1,0,0,0]] -- size of this vector is 3X6.\n",
    "```\n",
    "\n",
    "In general, each word of each document is converted to a vector.\n",
    "\n",
    "**Pros:** \n",
    "- Very simple, intuitive, easy to implement.\n",
    "    \n",
    "**Cons:** \n",
    "- Sparsity, different array sizes won't work, out of vocabulary words will throw an error, no capturing of semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb44cc1-d8c3-4fe4-b81c-e4f1cea5421a",
   "metadata": {},
   "source": [
    "### 2. Bag of Words:\n",
    "It deals with occurence of words in a document. (Unigram)\n",
    "\n",
    "- Doc1 = \"The more the merrier.\"\n",
    "- Doc2 = \"More people enrolled\"\n",
    "- Doc3 = \"Enrolled last week\"\n",
    "\n",
    "Vocabulary of this corpus = [\"The\",\"more\",\"merrier\",\"peope\",\"enrolled\",\"last\",\"week\"].\n",
    "In the vector form these docs can be converted as:\n",
    "    \n",
    "```\n",
    "Doc1 = [2,1,1,0,0,0,0]\n",
    "Doc2 = [0,1,0,1,1,0,0]\n",
    "Doc3 = [0,0,0,0,1,1,1]\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- Simple and easy to understand, No fixed size required, doesn't throw an error for out of vocabulary words, captures semantic meaning (less)\n",
    "\n",
    "**Cons:**\n",
    "- Ignores new words in the text dataset.\n",
    "- Sparsity\n",
    "- Consider: \"This is a good book\" and \"This is not a good book\". Because the docs contain same words at same frequency, this approach treats the two documents as similar, when in reality they convey opposite meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bb91dd6-c9a2-40cf-9fa3-b80609eeed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "675af4b4-59ba-42e9-a765-2e3e343e2bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'documents':[\"The more the merrier.\",\"More people enrolled\",\"Enrolled last week\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e680bb0f-7347-4082-9b7e-f5b48779b714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The more the merrier.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>More people enrolled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enrolled last week</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               documents\n",
       "0  The more the merrier.\n",
       "1   More people enrolled\n",
       "2     Enrolled last week"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b93221d1-78cf-451e-bb55-d8d23573f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3bb6207a-cbbd-46a8-9f8d-bae3ddfd759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = cv.fit_transform(df1['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ad336277-221d-40a3-9883-aea8ed9eea25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 5,\n",
       " 'more': 3,\n",
       " 'merrier': 2,\n",
       " 'people': 4,\n",
       " 'enrolled': 0,\n",
       " 'last': 1,\n",
       " 'week': 6}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vocabulary\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb12bfc-548b-4b46-bd64-adc6f0861204",
   "metadata": {},
   "source": [
    "The numbers indicate their ordering index. These words are alphabetically ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "de5e9ca0-0e49-4111-8f54-af82cc8d2bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector form of doc1: [[0 0 1 1 0 2 0]]\n",
      "Vector form of doc2: [[1 0 0 1 1 0 0]]\n",
      "Vector form of doc3: [[1 1 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Vector form of doc1:\",bow[0].toarray())\n",
    "print(\"Vector form of doc2:\",bow[1].toarray())\n",
    "print(\"Vector form of doc3:\",bow[2].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78ed3cd4-7b85-43ea-b895-ae3193d13793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform(['This sentence contains more words']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc7e5ff-5199-4ed1-891c-1cbbf18f9f11",
   "metadata": {},
   "source": [
    "As expected the new words got ignored in the vector form; one of the drawback of BoW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2321c0d2-beb9-4036-ad16-03dfbd726b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdb74989-39d7-4a4f-a4ea-01ef77b3d588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def removetags(text):\n",
    "    pattern = '<.*?>'\n",
    "    return re.sub(pattern,'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25a9abe6-8d13-4862-a385-3cb1bd912ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(removetags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09d12ed6-bed7-492b-8901-c644a6ce094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = cv.fit_transform(df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33b8890b-bb75-4dab-b1e6-0ee26256c667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104083"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9a508148-e2ee-40a9-a20e-cc34c040b6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "334596b1-71f3-4b83-982c-924c2ada9450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]]\n",
      "[[0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(bow[0].toarray())\n",
    "print(bow[1].toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab07e848-ddda-4d63-b10c-efe6bfde246c",
   "metadata": {},
   "source": [
    "Sparsity can be seen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd273570-ad1a-4492-90bc-04a23cd5f155",
   "metadata": {},
   "source": [
    "### 3. N-gram:\n",
    "The vocabulary is the combination of n-words.\n",
    "\n",
    "**Pros:**\n",
    "- Consider \"This book is very good\" and \"This book is not good\". Using Bi-gram, vocabulary is [\"This book\",\"book is\",\"is very\",\"very good\",\"is not\",\"not good\"].\n",
    "```\n",
    "Doc1 = [1,1,1,1,0,0]\n",
    "Doc2 = [1,1,0,0,1,1]\n",
    "```\n",
    "The vectors are now far from each other in the vector space which is why they are no longer similar.\n",
    "- Better conveying of semantic meaning.\n",
    "  \n",
    "**Cons:**\n",
    "- Dimension of vocabulary increases --> model interpretation increases, model computation increases.\n",
    "- Out of Vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4ba1f6b1-bd48-4425-ab6e-33468c94e9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_2 = CountVectorizer(ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98725631-6314-4ae7-8168-c0596d22be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = cv_2.fit_transform(df1['documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b11ab225-dcd6-4ba9-973c-9ab2d6256027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the more': 6,\n",
       " 'more the': 3,\n",
       " 'the merrier': 5,\n",
       " 'more people': 2,\n",
       " 'people enrolled': 4,\n",
       " 'enrolled last': 0,\n",
       " 'last week': 1}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_2.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153f796e-3efa-4aed-bda0-a86d62454607",
   "metadata": {},
   "source": [
    "ngram_range = (1,2) will contain words from unigram as well as bigram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d31012-0221-442e-a9e0-776f4a223e77",
   "metadata": {},
   "source": [
    " ### 4. TF-IDF\n",
    " Assigns weightage to words. \n",
    "\n",
    " **Term Frequency (TF)** : Number of times word w occurs in a given documensts / Total number of wordds in the document. <br>\n",
    " **Inverse Document Frequency** : log(Number of documents / Number of documents with word w) --> Captures the \"rare\" or \"frequency\" of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "941be29f-8059-425d-936a-f1b52fcbe6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The more the merrier.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>More people enrolled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enrolled last week</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               documents\n",
       "0  The more the merrier.\n",
       "1   More people enrolled\n",
       "2     Enrolled last week"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f1ee105b-6667-43e7-96ac-fff5edad0f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.42339448, 0.32200242, 0.        ,\n",
       "        0.84678897, 0.        ],\n",
       "       [0.51785612, 0.        , 0.        , 0.51785612, 0.68091856,\n",
       "        0.        , 0.        ],\n",
       "       [0.4736296 , 0.62276601, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.62276601]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit_transform(df1['documents']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ccaed935-65e6-433b-b906-6eb460808fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.28768207 1.69314718 1.69314718 1.28768207 1.69314718 1.69314718\n",
      " 1.69314718]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ca1eaf45-bf9c-43c2-a9a6-14a504563302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enrolled' 'last' 'merrier' 'more' 'people' 'the' 'week']\n"
     ]
    }
   ],
   "source": [
    "#Vocabulary\n",
    "print(tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e35e1f-d720-4f2c-86dd-e2918bde4bb4",
   "metadata": {},
   "source": [
    "**Why do we take log in the calculation of IDF?**\n",
    "\n",
    "If you calculate the IDF for a rare word in the document, the computation without log woulg be very high and will dominate the Term Frequency measure. Therefore, to ignore this situation, we take a log while computing IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d333544-a980-44d3-8d33-824a1a659c9a",
   "metadata": {},
   "source": [
    "**Pros:** - Used for Information retrieval. <br>\n",
    "**Cons:** - Sparsity, Out of Vocabulary, dimension becomes very big and does not capture the semantic meaning of a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117504f6-07e7-403e-bcd7-5b11bd9d4e21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
